Open up VS code

docker-compose down -v  # remove previous volumes
docker-compose up -d    # start fresh with persistence

cd C:\Users\joshu\Desktop\Rec_Model
docker cp C:\Users\joshu\Desktop\Rec_Model\user_personalized_features.csv rec_model_mysql_1:/var/lib/mysql-files/user_personalized_features.csv --> copy into the docker container

docker exec -it rec_model_mysql_1 mysql -u root -p --> -i (interactive) keeps STDIN open, -t gives a terminal like interface so I can interact with the shell, -u user, -p password.

SHOW VARIABLES LIKE 'secure_file_priv'; --> MySQL has a setting that restricts file paths. Use this to show from what path the load data infile can read.

SHOW DATABASES;
USE your_database;
SHOW TABLES;

ALTER TABLE your_table
MODIFY Age VARCHAR(10);
ALTER TABLE your_table
ADD COLUMN idx INT FIRST;


CREATE TABLE personal_features (
    idx INT,
    User_ID VARCHAR(10),
    Age INT,
    Gender VARCHAR(10),
    Location VARCHAR(100),
    Income DECIMAL(10,2),
    Interests TEXT,
    Last_Login_Days_Ago INT,
    Purchase_Frequency INT,
    Average_Order_Value DECIMAL(10,2),
    Total_Spending DECIMAL(12,2),
    Product_Category_Preference VARCHAR(100),
    Time_Spent_on_Site_Minutes INT,
    Pages_Viewed INT,
    Newsletter_Subscription VARCHAR(10)
);
#Loads data into table from csv file
LOAD DATA INFILE '/var/lib/mysql-files/cleaned_data.csv'
INTO TABLE your_table
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;

Enter the MySQL container. Created user 'admin' so root is not used in practice. Granted admin privileges to admin allowing the database to be brought up by that user in Spark. 

Apache Spark

http://localhost:8080

NEXT STEP CREATE SPARK CONTAINER AND LOAD FROM MYSQL DATABASE MODIFY THE DATA

#use this to run
spark-submit   --jars /app/mysql-connector-j-9.3.0.jar   /app/transform_data.py