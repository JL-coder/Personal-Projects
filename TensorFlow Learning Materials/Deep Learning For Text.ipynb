{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cdf8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip downloaded file\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b303c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    #Create seed\n",
    "    random.Random(1337).shuffle(files)\n",
    "    #Use 20% of training files for validation\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    #Move files to validation - neg/pos\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, \n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b081da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 3 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#tensorflow as a utility similar to their image dataset from directory for text\n",
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size = batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size = batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c40453c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (32,)\n",
      "input dtype:  <dtype: 'string'>\n",
      "targets shape:  (32,)\n",
      "targets dtype:  <dtype: 'int32'>\n",
      "inputs[0]:  tf.Tensor(b'I watched this film recently on DVD and I have to say I wasn\\'t impressed. I know it\\'s taboo to knock independent films, but this one felt devoid of entertainment.<br /><br />The premise was interesting, but the execution of it fell short. I found myself thinking \"okay, they\\'re just getting into it, the story will pick up soon\". Before I knew it, the film was over and the story never picked up. I can\\'t say I found the acting all that impressive either. It was pretty bad. Not Star Wars prequel trilogy bad, but bad nonetheless.<br /><br />I\\'m not sure what the running time was, I\\'ll assume two hours (because it\\'s a safe estimate). Anyway, when the film was finished, I felt as though I deserved some kind of recognition for the will power I exerted in not stopping the film and walking away halfway through.<br /><br />Again, I was thoroughly unimpressed, and eventually bored out of my wits. I\\'m not one of those guys who requires fast-paced action and explosions in a film, so don\\'t start in on me as that being a reason for not liking it.', shape=(), dtype=string)\n",
      "targets[0]:  tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#show shape and dtype of dataset\n",
    "for inputs, targets in train_ds:\n",
    "    print(\"input shape: \", inputs.shape)\n",
    "    print(\"input dtype: \", inputs.dtype)\n",
    "    print(\"targets shape: \", targets.shape)\n",
    "    print(\"targets dtype: \", targets.dtype)\n",
    "    print(\"inputs[0]: \", inputs[0])\n",
    "    print(\"targets[0]: \", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a43310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "#Test out multi-hot encoded binary word vector unigram\n",
    "#Use 20k most frequent words and encode as multi-hot binary vectors\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens = 20000,\n",
    "    output_mode = \"multi_hot\"\n",
    ")\n",
    "\n",
    "#dataset prep w/ only raw text inputs (no label)\n",
    "text_only_train_ds = train_ds.map(lambda x, y:x)\n",
    "#use dataset to index the dataset vocab via adapt() method\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "#prep processed versions of data. specifiy number of cpu cores\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb48918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    #Inputs are batchs of 20k vectors\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0381ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "  \n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a821c7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2344/2344 [==============================] - 65s 27ms/step - loss: -637.1317 - accuracy: 0.1667 - val_loss: 1769.6310 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "2344/2344 [==============================] - 10s 4ms/step - loss: -3838.2576 - accuracy: 0.1667 - val_loss: 6442.3540 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "2344/2344 [==============================] - 11s 5ms/step - loss: -9936.0264 - accuracy: 0.1667 - val_loss: 14017.4561 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "2344/2344 [==============================] - 10s 4ms/step - loss: -18817.6758 - accuracy: 0.1667 - val_loss: 24467.3164 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -30732.8984 - accuracy: 0.1667 - val_loss: 37810.1328 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -45367.4258 - accuracy: 0.1667 - val_loss: 54041.8945 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -62542.5547 - accuracy: 0.1667 - val_loss: 73150.4922 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -83173.1406 - accuracy: 0.1667 - val_loss: 95220.4062 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -107058.5547 - accuracy: 0.1667 - val_loss: 120515.5703 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -133101.7656 - accuracy: 0.1667 - val_loss: 148961.1719 - val_accuracy: 0.5000\n",
      "782/782 [==============================] - 29s 37ms/step - loss: 1760.2189 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "#Cache the datasets in memory so that preprocessing is only done once during firt epoch\n",
    "#Preprocessed data is reused\n",
    "model.fit(binary_1gram_train_ds.cache(),                   \n",
    "          validation_data=binary_1gram_val_ds.cache(),     \n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\") \n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b01873",
   "metadata": {},
   "source": [
    "### Reconfigure For Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f5a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams = 2,\n",
    "                                      max_tokens = 20000,\n",
    "                                      output_mode = \"multi_hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4c4b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2344/2344 [==============================] - 24s 10ms/step - loss: -746.5081 - accuracy: 0.1666 - val_loss: 2102.5300 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -4649.4814 - accuracy: 0.1667 - val_loss: 7813.5186 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -12121.1641 - accuracy: 0.1667 - val_loss: 17114.9668 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -23142.3047 - accuracy: 0.1667 - val_loss: 29989.9434 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -37751.4375 - accuracy: 0.1667 - val_loss: 46458.5391 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -55870.6523 - accuracy: 0.1667 - val_loss: 66557.4844 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -77909.1953 - accuracy: 0.1667 - val_loss: 90257.4453 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -103142.7500 - accuracy: 0.1667 - val_loss: 117517.7891 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "2344/2344 [==============================] - 10s 4ms/step - loss: -131171.9531 - accuracy: 0.1667 - val_loss: 148295.7500 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "2344/2344 [==============================] - 9s 4ms/step - loss: -165103.0000 - accuracy: 0.1667 - val_loss: 182785.2344 - val_accuracy: 0.5000\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 2090.1511 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "#Train/test bigram model\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    " \n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d75ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count word occurrences in text\n",
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14709fb7",
   "metadata": {},
   "source": [
    "### TF-IDF Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c92cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Math example\n",
    "def tfidf(term, document, dataset):\n",
    "    term_freq = document.count(term)\n",
    "    doc_freq = math.log(sum(doc.count(term) for doc in dataset) + 1)\n",
    "    return term_freq / doc_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ec7f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Built into textvectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b99d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Model - Not working right now!\n",
    "#.adapt() learns TF-IDF weights in addition to the vocab\n",
    "text_vectorization.adapt(text_only_train_ds)    \n",
    " \n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    " \n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac630d",
   "metadata": {},
   "source": [
    "### Exporting a Model that Processes Raw Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For production models can reuse the TextVectorization layer and add to trained model\n",
    "inputs = keras.Input(shape=(1,), dtype=\"string\")   \n",
    "processed_inputs = text_vectorization(inputs)      \n",
    "outputs = model(processed_inputs)                  \n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8504d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data) \n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc5897",
   "metadata": {},
   "source": [
    "## Sequence Model Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9064f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "  \n",
    "max_length = 600 \n",
    "max_tokens = 20000 \n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,     \n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    " \n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "028f9bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "tf.one_hot (TFOpLambda)      (None, None, 20000)       0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                5128448   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,128,513\n",
      "Trainable params: 5,128,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Sequence LSTM Model built on one-hot encoded vector sequences\n",
    "import tensorflow as tf\n",
    "#one input is a sequence of ints\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")   \n",
    "#Encode the int into binary 20k dimensional vectors\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "#Add bidirectional LSTM\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)   \n",
    "x = layers.Dropout(0.5)(x) \n",
    "#Classification layer\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)    \n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Too large to run. each input matrix is of size (600,20000) i.e.\n",
    "#600 words per sample w/ 20,000 possible words -- better to use word embeddings\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\") \n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf6b6b",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc3d00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90ff5363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 256)         5120000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                73984     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2344/2344 [==============================] - 183s 75ms/step - loss: -40.1637 - accuracy: 0.1667 - val_loss: 75.2857 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "2344/2344 [==============================] - 189s 81ms/step - loss: -110.1410 - accuracy: 0.1667 - val_loss: 145.2160 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "2344/2344 [==============================] - 180s 77ms/step - loss: -180.0487 - accuracy: 0.1667 - val_loss: 215.2051 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "2344/2344 [==============================] - 234s 100ms/step - loss: -250.1262 - accuracy: 0.1667 - val_loss: 285.0866 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "2344/2344 [==============================] - 261s 111ms/step - loss: -320.1653 - accuracy: 0.1667 - val_loss: 355.1201 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "2344/2344 [==============================] - 264s 112ms/step - loss: -390.2225 - accuracy: 0.1667 - val_loss: 425.0183 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "2344/2344 [==============================] - 262s 112ms/step - loss: -459.9780 - accuracy: 0.1667 - val_loss: 495.1705 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "2344/2344 [==============================] - 261s 111ms/step - loss: -530.1517 - accuracy: 0.1667 - val_loss: 565.1691 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "2344/2344 [==============================] - 263s 112ms/step - loss: -600.6881 - accuracy: 0.1667 - val_loss: 635.0973 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "2344/2344 [==============================] - 262s 112ms/step - loss: -670.1516 - accuracy: 0.1667 - val_loss: 705.1334 - val_accuracy: 0.5000\n",
      "782/782 [==============================] - 51s 64ms/step - loss: 75.2857 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "#Deep learning model with embedding layer trained from scratch\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "  \n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\") \n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df2a44",
   "metadata": {},
   "source": [
    "### Masking Embedded Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98006948",
   "metadata": {},
   "source": [
    "#### Our input contains words that are padded at the end and our RNN learns bidirectionally so when it is learning one of these words backwards it has alot of zeros. Can use masking which attaches metadata to the word telling the model to ignore these zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06aedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masking enabled\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "    #Set mask_zero to true!\n",
    "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\") \n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67364c",
   "metadata": {},
   "source": [
    "### Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff39d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example layer that computes attention scores. \n",
    "num_heads = 4 \n",
    "embed_dim = 256 \n",
    "mha_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "outputs = mha_layer(inputs, inputs, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3853e",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b291c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000 \n",
    "embed_dim = 256 \n",
    "num_heads = 2 \n",
    "dense_dim = 32 \n",
    "  \n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "#Returns full sequences neeed to reduce to a single vector for classification via pooling.\n",
    "x = layers.GlobalMaxPooling1D()(x)                          \n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
