{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849aca36",
   "metadata": {},
   "source": [
    "### Text Generation With Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6328a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-08-15 12:46:23--  ftp://https//\n",
      "           => '.listing'\n",
      "Resolving https (https)... failed: No such host is known. .\n",
      "wget: unable to resolve host address 'https'\n",
      "/ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz: Scheme missing.\n"
     ]
    }
   ],
   "source": [
    "#download and uncompress movie reviews\n",
    "!wget https:/ /ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97b8de20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105006 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#Strip out HTML tags that occur in reviews\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef45cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep TextVectorization layer\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "  \n",
    "sequence_length = 100 \n",
    "vocab_size = 15000                            \n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,                \n",
    "    output_mode=\"int\",                        \n",
    "    output_sequence_length=sequence_length,   \n",
    ")\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a4c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lm_dataset(text_batch):\n",
    "    #convert batch of text (strings) to batch of int sequences\n",
    "    vectorized_sequences = text_vectorization(text_batch)  \n",
    "    #Create inputs by cutting off last word of sequences\n",
    "    x = vectorized_sequences[:, :-1]                    \n",
    "    #create targets by offsetting the sequences by 1\n",
    "    y = vectorized_sequences[:, 1:]                          \n",
    "    return x, y\n",
    "  \n",
    "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a22a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):  \n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(                          \n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)              \n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions                        \n",
    " \n",
    "    def compute_mask(self, inputs, mask=None):                             \n",
    "        return tf.math.not_equal(inputs, 0)                                \n",
    " \n",
    "    def get_config(self):                                                  \n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f028f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True                     \n",
    "  \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3efcde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simple transformer-based language model\n",
    "from tensorflow.keras import layers\n",
    "embed_dim = 256 \n",
    "latent_dim = 2048 \n",
    "num_heads = 2 \n",
    "  \n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)       \n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text-generation callback that has a \"temp\" gauge for amount of randomness in model\n",
    "import numpy as np\n",
    "#Dictonary that maps word indices back to strings, for text decoding\n",
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
    "#Temp sampling from prob dist\n",
    "def sample_next(predictions, temperature=1.0):                        \n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "  \n",
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self,\n",
    "                 #prompt used for seed generation\n",
    "                 prompt,        \n",
    "                 #how many words to use\n",
    "                 generate_length,                                      \n",
    "                 model_input_length,\n",
    "                 #range of temp for sampling\n",
    "                 temperatures=(1.,),                                   \n",
    "                 print_freq=1):\n",
    "        self.prompt = prompt\n",
    "        self.generate_length = generate_length\n",
    "        self.model_input_length = model_input_length\n",
    "        self.temperatures = temperatures\n",
    "        self.print_freq = print_freq\n",
    "  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.print_freq != 0:\n",
    "            return\n",
    "        for temperature in self.temperatures:\n",
    "            print(\"== Generating with temperature\", temperature)\n",
    "            #when generate text start from prompt\n",
    "            sentence = self.prompt                                     \n",
    "            for i in range(self.generate_length):\n",
    "                #Feed current sequence to model\n",
    "                tokenized_sentence = text_vectorization([sentence])    \n",
    "                predictions = self.model(tokenized_sentence)           \n",
    "                #Get predictions for the last timestep and use to sample a new word\n",
    "                next_token = sample_next(predictions[0, i, :])         \n",
    "                sampled_token = tokens_index[next_token]        \n",
    "                #Append word to sample\n",
    "                sentence += \" \" + sampled_token                        \n",
    "            print(sentence)\n",
    "            \n",
    "prompt = \"This movie\" \n",
    "text_gen_callback = TextGenerator(\n",
    "    prompt,\n",
    "    generate_length=50,\n",
    "    model_input_length=sequence_length,\n",
    "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(lm_dataset, epochs=200, callbacks=[text_gen_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
